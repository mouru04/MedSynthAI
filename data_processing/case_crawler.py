"""
çˆ±çˆ±åŒ»ç—…å†æ•°æ®é‡‡é›†æ¨¡å— - ç—…ä¾‹è¯¦æƒ…çˆ¬å–æ¨¡å—

ä½¿ç”¨ JsonCssExtractionStrategy è¿›è¡Œç»“æ„åŒ–æ•°æ®æå–
"""

import asyncio
import json
from typing import Dict, List, Optional, Union
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

try:
    from .schemas import get_case_extraction_schema, get_simple_case_extraction_schema
    from .utils import (
        create_content_filter,
        extract_case_id_from_url,
        load_urls_from_file,
        save_case_data_to_json
    )
except ImportError:
    from schemas import get_case_extraction_schema, get_simple_case_extraction_schema
    from utils import (
        create_content_filter,
        extract_case_id_from_url,
        load_urls_from_file,
        save_case_data_to_json
    )


async def crawl_case_details_improved(
    url_file: str = "iiyi_case_urls.txt",
    output_dir: str = "case_details",
    max_concurrent: int = 3,
    start_index: int = 0,
    end_index: Optional[int] = None,
    verbose: bool = True
) -> Dict[str, Union[int, List[str]]]:
    """
    æ”¹è¿›çš„ç—…ä¾‹è¯¦æƒ…çˆ¬å–å‡½æ•°
    
    ä½¿ç”¨ JsonCssExtractionStrategy è¿›è¡Œç»“æ„åŒ–æ•°æ®æå–
    ç›´æ¥ä¿å­˜ä¸ºJSONæ ¼å¼è€Œä¸æ˜¯markdown
    
    Args:
        url_file: URLæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        start_index: èµ·å§‹URLç´¢å¼•
        end_index: ç»“æŸURLç´¢å¼•
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
    Returns:
        åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    
    if verbose:
        print("ğŸ” å¼€å§‹çˆ¬å–ç—…ä¾‹è¯¦æƒ…é¡µ (æ”¹è¿›ç‰ˆ)...")

    # ========== ç¬¬ä¸€é˜¶æ®µï¼šåŠ è½½URLåˆ—è¡¨ ==========
    all_urls = load_urls_from_file(url_file)

    if end_index is None:
        end_index = len(all_urls)

    urls_to_crawl = all_urls[start_index:end_index]

    if verbose:
        print(f"ğŸ“„ æ€»è®¡ {len(all_urls)} ä¸ªURLï¼Œæœ¬æ¬¡çˆ¬å– {len(urls_to_crawl)} ä¸ª "
              f"(ç´¢å¼• {start_index} åˆ° {end_index-1})")

    # ========== ç¬¬äºŒé˜¶æ®µï¼šåˆ›å»ºæå–ç­–ç•¥ ==========
    # å°è¯•ä¸»schemaï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•ç®€åŒ–schema
    schemas = [get_case_extraction_schema(), get_simple_case_extraction_schema()]
    
    failed_urls: List[str] = []
    success_count = 0

    async with AsyncWebCrawler() as crawler:
        # é…ç½®markdownç”Ÿæˆå™¨ - ä½¿ç”¨å†…å®¹è¿‡æ»¤å™¨
        content_filter = create_content_filter()
        md_generator = DefaultMarkdownGenerator(
            content_filter=content_filter,
            options={
                "ignore_links": False,
                "escape_html": False
            }
        )

        if verbose:
            print(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– (æœ€å¤§å¹¶å‘æ•°: {max_concurrent})...")

        # åˆ†æ‰¹çˆ¬å–ä»¥æ§åˆ¶å¹¶å‘
        for batch_start in range(0, len(urls_to_crawl), max_concurrent):
            batch_end = min(batch_start + max_concurrent, len(urls_to_crawl))
            batch_urls = urls_to_crawl[batch_start:batch_end]

            if verbose:
                print(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_start//max_concurrent + 1}: "
                      f"çˆ¬å– {len(batch_urls)} ä¸ªURL "
                      f"({batch_start+1}-{batch_end}/{len(urls_to_crawl)})")

            # æ‰¹é‡çˆ¬å–
            results = await crawler.arun_many(batch_urls, config=None)

            # ========== ç¬¬å››é˜¶æ®µï¼šå¤„ç†ç»“æœ ==========
            for i, result in enumerate(results):
                url = batch_urls[i]
                case_id = extract_case_id_from_url(url)

                if not result.success:
                    if verbose:
                        print(f"  âŒ å¤±è´¥: {case_id} - {result.error_message}")
                    failed_urls.append(url)
                    continue

                extracted_data = {}
                
                try:
                    # å°è¯•ä½¿ç”¨ç»“æ„åŒ–æå–
                    extraction_success = False
                    for schema_idx, schema in enumerate(schemas):
                        try:
                            extraction_config = CrawlerRunConfig(
                                extraction_strategy=JsonCssExtractionStrategy(schema),
                                markdown_generator=md_generator,
                                verbose=False
                            )
                            
                            extraction_result = await crawler.arun(url, config=extraction_config)
                            
                            if extraction_result.success and hasattr(extraction_result, 'extracted_content'):
                                extracted_data = json.loads(extraction_result.extracted_content)
                                
                                extraction_success = True
                                if verbose and schema_idx > 0:
                                    print(f"  âš ï¸ ä¸»Schemaå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨SchemaæˆåŠŸ: {case_id}")
                                break
                                
                        except Exception as e:
                            if verbose and schema_idx == 0:
                                print(f"  âš ï¸ ä¸»Schemaå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨Schema: {case_id} - {str(e)}")
                            continue

                    # ========== ç›´æ¥ä¿å­˜ä¸ºJSONæ ¼å¼ ==========
                    # ä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨æå–çš„ç»“æ„åŒ–æ•°æ®
                    output_file = save_case_data_to_json(
                        url=url,
                        case_id=case_id,
                        extracted_data=extracted_data[0] if extracted_data else {},
                        extraction_success=extraction_success,
                        output_dir=output_dir
                    )

                    success_count += 1

                    if verbose:
                        print(f"  âœ… æˆåŠŸ: {case_id} â†’ {output_file.name}")

                except Exception as e:
                    if verbose:
                        print(f"  âš ï¸ å¤„ç†å¤±è´¥: {case_id} - {str(e)}")
                    failed_urls.append(url)

    # ========== ç¬¬äº”é˜¶æ®µï¼šç»Ÿè®¡ä¿¡æ¯ ==========
    stats = {
        "total": len(urls_to_crawl),
        "success": success_count,
        "failed": len(failed_urls),
        "failed_urls": failed_urls
    }

    if verbose:
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–å®Œæˆç»Ÿè®¡ (æ”¹è¿›ç‰ˆ)")
        print("=" * 60)
        print(f"âœ… æˆåŠŸ: {stats['success']}/{stats['total']} "
              f"({stats['success']/stats['total']*100:.1f}%)")
        print(f"âŒ å¤±è´¥: {stats['failed']}/{stats['total']}")
        print(f"ğŸ“ è¾“å‡ºæ ¼å¼: JSON (ç»“æ„åŒ–æ•°æ®)")

        if failed_urls:
            print(f"\nå¤±è´¥çš„URL (å‰5ä¸ª):")
            for i, url in enumerate(failed_urls[:5], 1):
                print(f"  {i}. {url}")

    return stats


async def main_crawl_details_improved(
    url_file: str = "iiyi_case_urls.txt",
    output_dir: str = "case_details",
    max_concurrent: int = 3,
    start_index: int = 0,
    end_index: int = 3,
    verbose: bool = True
):
    """
    æ”¹è¿›çš„ç—…ä¾‹è¯¦æƒ…çˆ¬å–ä¸»å‡½æ•°
    
    Args:
        url_file: URLæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        start_index: èµ·å§‹URLç´¢å¼•
        end_index: ç»“æŸURLç´¢å¼•
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    print("=" * 60)
    print("çˆ±çˆ±åŒ»ç—…å†è¯¦æƒ…çˆ¬å–å·¥å…· (æ”¹è¿›ç‰ˆ)")
    print("=" * 60)

    stats = await crawl_case_details_improved(
        url_file=url_file,
        output_dir=output_dir,
        max_concurrent=max_concurrent,
        start_index=start_index,
        end_index=end_index,
        verbose=verbose
    )

    print(f"\næ€»è®¡: {stats['total']} ä¸ªURL")
    print(f"æˆåŠŸ: {stats['success']} ä¸ª")
    print(f"å¤±è´¥: {stats['failed']} ä¸ª")
    print(f"ğŸ“ è¾“å‡ºæ ¼å¼: JSON (ç»“æ„åŒ–æ•°æ®)")
