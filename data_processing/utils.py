"""
çˆ±çˆ±åŒ»ç—…å†æ•°æ®é‡‡é›†æ¨¡å— - å·¥å…·å‡½æ•°

åŒ…å«å†…å®¹æ¸…ç†ã€URLå¤„ç†ã€æ–‡ä»¶æ“ä½œç­‰é€šç”¨å·¥å…·å‡½æ•°
"""

import re
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Union, Optional, Set
from crawl4ai.content_filter_strategy import PruningContentFilter


def create_content_filter():
    """åˆ›å»ºä¼˜åŒ–çš„å†…å®¹è¿‡æ»¤å™¨"""
    return PruningContentFilter(
        threshold=0.45,           # å†…å®¹å¯†åº¦é˜ˆå€¼
        threshold_type="dynamic", # åŠ¨æ€é˜ˆå€¼
        min_word_threshold=3      # æœ€å°‘è¯æ•°
    )


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬å†…å®¹"""
    if not text:
        return ""
    
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text.strip())
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    return text


def extract_publisher_from_structured_data(data: Dict) -> str:
    """ä»ç»“æ„åŒ–æ•°æ®ä¸­æå–å‘å¸ƒäººä¿¡æ¯"""
    publisher_parts = []
    
    # æå–å§“å
    if 'publisher_name' in data and data['publisher_name']:
        publisher_parts.append(data['publisher_name'])
    
    # æå–èŒç§°
    if 'publisher_title' in data and data['publisher_title']:
        publisher_parts.append(data['publisher_title'])
    
    # æå–æ›´æ–°æ—¶é—´
    if 'publisher_update_time' in data and data['publisher_update_time']:
        time_match = re.search(r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})', data['publisher_update_time'])
        if time_match:
            publisher_parts.append(f"æ›´æ–°æ—¶é—´ï¼š{time_match.group(1)}")
    
    return " | ".join(publisher_parts) if publisher_parts else "å‘å¸ƒäººä¿¡æ¯æå–å¤±è´¥"


def format_case_summary_structured(data: Dict) -> str:
    """æ ¼å¼åŒ–ç»“æ„åŒ–çš„ç—…ä¾‹æ‘˜è¦"""
    summary_parts = []
    
    # å¤„ç†ç»“æ„åŒ–çš„ç—…ä¾‹æ‘˜è¦
    if 'case_summary_structured' in data and data['case_summary_structured']:
        for item in data['case_summary_structured']:
            if isinstance(item, dict) and 'label' in item and 'content' in item:
                summary_parts.append(f"{item['label']} {item['content']}")
    
    # å¦‚æœæ²¡æœ‰ç»“æ„åŒ–æ•°æ®ï¼Œå°è¯•ä»æ™®é€šæ–‡æœ¬ä¸­æå–
    if not summary_parts and 'case_summary' in data:
        summary_text = data['case_summary']
        # å°è¯•æå–å…³é”®ä¿¡æ¯
        patterns = {
            'åŸºæœ¬ä¿¡æ¯': r'ã€åŸºæœ¬ä¿¡æ¯ã€‘([^ã€]+)',
            'å‘ç—…åŸå› ': r'ã€å‘ç—…åŸå› ã€‘([^ã€]+)',
            'ä¸´åºŠè¯Šæ–­': r'ã€ä¸´åºŠè¯Šæ–­ã€‘([^ã€]+)',
            'æ²»ç–—æ–¹æ¡ˆ': r'ã€æ²»ç–—æ–¹æ¡ˆã€‘([^ã€]+)',
            'æ²»ç–—ç»“æœ': r'ã€æ²»ç–—ç»“æœã€‘([^ã€]+)',
            'ç—…æ¡ˆé‡ç‚¹': r'ã€ç—…æ¡ˆé‡ç‚¹ã€‘([^ã€]+)'
        }
        
        for label, pattern in patterns.items():
            match = re.search(pattern, summary_text)
            if match:
                summary_parts.append(f"{label}ï¼š{match.group(1).strip()}")
    
    return "\n".join(summary_parts) if summary_parts else "ç—…ä¾‹æ‘˜è¦æå–å¤±è´¥"


def extract_case_urls_from_html(html: str) -> List[str]:
    """ä»HTMLä¸­æå–ç—…å†URL"""
    case_urls: Set[str] = set()

    # åŒ¹é…å„ç§å¯èƒ½çš„URLæ ¼å¼
    patterns = [
        r'https?://bingli\.iiyi\.com/show/[^"\'<>\s]+\.html',
        r'//bingli\.iiyi\.com/show/[^"\'<>\s]+\.html',
        r'/show/[^"\'<>\s]+\.html',
    ]

    for pattern in patterns:
        matches = re.findall(pattern, html)
        for match in matches:
            # è§„èŒƒåŒ–URL
            if match.startswith('//'):
                url = 'https:' + match
            elif match.startswith('/show/'):
                url = 'https://bingli.iiyi.com' + match
            else:
                url = match

            # éªŒè¯URLæ ¼å¼ï¼šå¿…é¡»åŒ…å«"-"
            filename_match = re.search(r'/show/([^/]+)\.html', url)
            if filename_match:
                filename = filename_match.group(1)
                if '-' in filename:
                    case_urls.add(url)

    return list(case_urls)


def extract_case_id_from_url(url: str) -> str:
    """ä»URLæå–ç—…ä¾‹ID"""
    match = re.search(r'/show/([^/]+)\.html', url)
    if match:
        return match.group(1)
    return str(hash(url))


async def save_case_urls_to_file(
    urls: List[str],
    output_file: str = "iiyi_case_urls.txt"
) -> None:
    """ä¿å­˜URLåˆ°æ–‡ä»¶"""
    with open(output_file, "w", encoding="utf-8") as f:
        for url in urls:
            f.write(f"{url}\n")

    print(f"ğŸ’¾ å·²ä¿å­˜ {len(urls)} ä¸ª URL åˆ° {output_file}")


def load_urls_from_file(url_file: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½URL"""
    with open(url_file, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    return urls


def save_case_data_to_json(
    url: str,
    case_id: str,
    extracted_data: Dict,
    extraction_success: bool,
    output_dir: str = "case_details"
) -> Path:
    """ä¿å­˜ç—…ä¾‹æ•°æ®ä¸ºJSONæ–‡ä»¶"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # å‡†å¤‡JSONæ•°æ®
    json_data = {
        "url": url,
        "case_id": case_id,
        "extracted_data": extracted_data,
        "extraction_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "extraction_success": extraction_success,
        "data_source": "çˆ±çˆ±åŒ» (iiyi.com)"
    }
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    output_file = output_path / f"{case_id}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    return output_file
