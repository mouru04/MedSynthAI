"""
çˆ±çˆ±åŒ»ç—…å†æ•°æ®é‡‡é›†æ¨¡å— - URLé‡‡é›†æ¨¡å—

è´Ÿè´£ä»çˆ±çˆ±åŒ»ç½‘ç«™é‡‡é›†æ‰€æœ‰ç—…å†URLåˆ—è¡¨
"""

import asyncio
from typing import List, Optional, Set
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

try:
    from .config import LIST_PAGE_PATTERN
    from .utils import extract_case_urls_from_html, save_case_urls_to_file
except ImportError:
    from config import LIST_PAGE_PATTERN
    from utils import extract_case_urls_from_html, save_case_urls_to_file


async def fetch_all_case_urls(
    start_page: int = 1,
    end_page: Optional[int] = None,
    max_pages: int = 100,
    verbose: bool = True
) -> List[str]:
    """
    è·å–çˆ±çˆ±åŒ»ç½‘ç«™çš„æ‰€æœ‰ç—…å†URL
    
    Args:
        start_page: èµ·å§‹é¡µç 
        end_page: ç»“æŸé¡µç ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
        max_pages: æœ€å¤§é¡µæ•°é™åˆ¶
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
    Returns:
        ç—…å†URLåˆ—è¡¨
    """
    if verbose:
        print("ğŸ” å¼€å§‹è·å–çˆ±çˆ±åŒ»ç—…å† URL...")

    case_urls: Set[str] = set()

    # ========== ç¬¬ä¸€é˜¶æ®µï¼šç¡®å®šé¡µé¢èŒƒå›´ ==========
    if end_page is None:
        if verbose:
            print("ğŸ” è‡ªåŠ¨æ¢æµ‹æœ€åä¸€é¡µ...")
        end_page = await _detect_last_page(start_page, max_pages, verbose)
        if verbose:
            print(f"âœ… æ£€æµ‹åˆ°æœ€åä¸€é¡µ: ç¬¬ {end_page} é¡µ")

    # é™åˆ¶æœ€å¤§é¡µæ•°
    if end_page - start_page + 1 > max_pages:
        if verbose:
            print(f"âš ï¸ é¡µé¢èŒƒå›´è¶…è¿‡æœ€å¤§é™åˆ¶ {max_pages}ï¼Œå°†åªçˆ¬å–å‰ {max_pages} é¡µ")
        end_page = start_page + max_pages - 1

    total_pages = end_page - start_page + 1
    if verbose:
        print(f"ğŸ“„ å°†çˆ¬å– {total_pages} ä¸ªåˆ—è¡¨é¡µ (ç¬¬ {start_page} é¡µåˆ°ç¬¬ {end_page} é¡µ)")

    # ========== ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡çˆ¬å–åˆ—è¡¨é¡µ ==========
    async with AsyncWebCrawler() as crawler:
        # ç”Ÿæˆæ‰€æœ‰åˆ—è¡¨é¡µURL
        list_page_urls = [
            LIST_PAGE_PATTERN.format(page=page)
            for page in range(start_page, end_page + 1)
        ]

        # é…ç½®çˆ¬è™«
        crawl_config = CrawlerRunConfig(
            only_text=False,
            verbose=verbose
        )

        if verbose:
            print(f"\nğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(list_page_urls)} ä¸ªåˆ—è¡¨é¡µ...")

        # æ‰¹é‡çˆ¬å–æ‰€æœ‰åˆ—è¡¨é¡µ
        results = await crawler.arun_many(list_page_urls, config=crawl_config)

        # ========== ç¬¬ä¸‰é˜¶æ®µï¼šæå–ç—…å†é“¾æ¥ ==========
        page_count = 0
        for result in results:
            page_count += 1

            if not result.success:
                if verbose:
                    print(f"âš ï¸ ç¬¬ {page_count} é¡µçˆ¬å–å¤±è´¥: {result.url}")
                continue

            # ä»HTMLä¸­æå–æ‰€æœ‰ç—…å†è¯¦æƒ…é¡µé“¾æ¥
            case_links = extract_case_urls_from_html(result.html)
            case_urls.update(case_links)

            if verbose:
                print(f"âœ“ ç¬¬ {page_count}/{total_pages} é¡µ: å‘ç° {len(case_links)} ä¸ªç—…å†é“¾æ¥ "
                      f"(ç´¯è®¡ {len(case_urls)} ä¸ª)")

    # ========== ç¬¬å››é˜¶æ®µï¼šè½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº ==========
    final_urls = sorted(list(case_urls))

    if verbose:
        print(f"\nâœ… å®Œæˆï¼å…±å‘ç° {len(final_urls)} ä¸ªå”¯ä¸€ç—…å† URL")

    return final_urls


async def _detect_last_page(
    start_page: int = 1,
    max_pages: int = 100,
    verbose: bool = False
) -> int:
    """
    æ£€æµ‹æœ€åä¸€é¡µ
    
    Args:
        start_page: èµ·å§‹é¡µç 
        max_pages: æœ€å¤§æ£€æµ‹é¡µæ•°
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
    Returns:
        æœ€åä¸€é¡µçš„é¡µç 
    """
    async def _page_has_cases(page_num: int) -> bool:
        """æ£€æŸ¥æŒ‡å®šé¡µç æ˜¯å¦åŒ…å«ç—…å†"""
        url = LIST_PAGE_PATTERN.format(page=page_num)

        async with AsyncWebCrawler() as crawler:
            config = CrawlerRunConfig(verbose=False)
            result = await crawler.arun(url, config=config)

            if not result.success:
                return False

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç—…å†é“¾æ¥
            case_links = extract_case_urls_from_html(result.html)
            return len(case_links) > 0

    # äºŒåˆ†æŸ¥æ‰¾æœ€åä¸€é¡µ
    left = start_page
    right = start_page + max_pages
    last_valid_page = start_page

    while left <= right:
        mid = (left + right) // 2

        if verbose:
            print(f"  æ£€æŸ¥ç¬¬ {mid} é¡µ...")

        has_cases = await _page_has_cases(mid)

        if has_cases:
            last_valid_page = mid
            left = mid + 1
        else:
            right = mid - 1

    return last_valid_page


async def main_fetch_urls(start_page: int = 1, max_pages: int = 5, verbose: bool = True):
    """
    é‡‡é›†URLåˆ—è¡¨çš„ä¸»å‡½æ•°
    
    Args:
        start_page: èµ·å§‹é¡µç 
        max_pages: æœ€å¤§é¡µæ•°
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    print("=" * 60)
    print("çˆ±çˆ±åŒ»ç—…å† URL é‡‡é›†å·¥å…·")
    print("=" * 60)

    case_urls = await fetch_all_case_urls(
        start_page=start_page,
        end_page=None,
        max_pages=max_pages,
        verbose=verbose
    )

    if case_urls:
        await save_case_urls_to_file(case_urls, "iiyi_case_urls.txt")
        print(f"\næ€»è®¡å‘ç°: {len(case_urls)} ä¸ªå”¯ä¸€ç—…å† URL")
